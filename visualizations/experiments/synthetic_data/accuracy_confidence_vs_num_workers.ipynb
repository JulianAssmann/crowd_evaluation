{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datasets import SyntheticDataset\n",
    "from crowd_evaluation import OldEvaluator, ConfidenceEvaluatorC, MajorityEvaluator, VoteAggregator\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "from visualizations.utils import say"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from joblib import Memory, Parallel, delayed\n",
    "memory = Memory('accuracy_vs_num_workers', verbose=0)\n",
    "\n",
    "@memory.cache\n",
    "def calc_fraction_of_wrong_interval_estimates(\n",
    "        num_samples: int,\n",
    "        num_workers: int,\n",
    "        confidence: float,\n",
    "        evaluator_name: str,\n",
    "        iter_count: int,\n",
    "        error_rates: list[int]):\n",
    "\n",
    "    correct_interval_estimates = np.zeros(iter_count * num_workers, dtype=np.float32)\n",
    "    error_rate_estimation_errors = np.zeros(iter_count * num_workers, dtype=np.float32)\n",
    "    int_sizes = np.zeros(iter_count * num_workers, dtype=np.float32)\n",
    "\n",
    "    # Saves for every sample generated whether the\n",
    "    # majority vote estimates the label correctly (=1) or not (=0)\n",
    "    majority_vote_estimations = np.zeros(iter_count * num_samples)\n",
    "\n",
    "    # Saves for every sample generated whether the\n",
    "    # weighted vote estimates the label correctly (=1) or not (=0)\n",
    "    weighted_vote_estimations = np.zeros(iter_count * num_samples)\n",
    "\n",
    "    for i in range(iter_count):\n",
    "        p_true = np.random.choice(error_rates, size=num_workers)\n",
    "        dataset = SyntheticDataset(num_samples=num_samples, num_workers=num_workers, p_true=p_true)\n",
    "\n",
    "        if evaluator_name == 'old':\n",
    "            evaluator = OldEvaluator(dataset)\n",
    "            ps, confs = evaluator.evaluate_workers_with_confidence(\n",
    "                dataset.workers,\n",
    "                confidence=confidence,\n",
    "                method='exhaustive'\n",
    "            )\n",
    "        elif evaluator_name == 'old greedy':\n",
    "            evaluator = OldEvaluator(dataset)\n",
    "            ps, confs = evaluator.evaluate_workers_with_confidence(\n",
    "                dataset.workers,\n",
    "                confidence=confidence,\n",
    "                method='greedy'\n",
    "            )\n",
    "        elif evaluator_name == 'majority':\n",
    "            evaluator = MajorityEvaluator(dataset)\n",
    "            ps = evaluator.evaluate_workers(dataset.workers)\n",
    "            confs = np.zeros(num_workers)\n",
    "        else:\n",
    "            evaluator = ConfidenceEvaluatorC(dataset)\n",
    "            ps, confs = evaluator.evaluate_workers_with_confidence(\n",
    "                dataset.workers,\n",
    "                confidence=confidence,\n",
    "            )\n",
    "\n",
    "\n",
    "        ground_truth = dataset.get_ground_truth_for_samples(dataset.samples)\n",
    "        maj_vote = VoteAggregator.majority_vote(dataset, dataset.samples)\n",
    "        weighted_vote = VoteAggregator.weighted_vote(dataset, dataset.samples, ps, 0.5)\n",
    "\n",
    "        min_limit, max_limit = ps-confs, ps+confs\n",
    "        correct_interval_estimates[i*num_workers:(i+1)*num_workers] = np.where((min_limit <= p_true) & (p_true <= max_limit), 1, 0)\n",
    "        int_sizes[i*num_workers:(i+1)*num_workers] = 2*confs\n",
    "        error_rate_estimation_errors[i*num_workers:(i+1)*num_workers] = np.abs(ps - p_true)\n",
    "\n",
    "        majority_vote_estimations[i*num_samples:(i+1)*num_samples] = (ground_truth == maj_vote)\n",
    "        weighted_vote_estimations[i*num_samples:(i+1)*num_samples] = (ground_truth == weighted_vote)\n",
    "\n",
    "    return correct_interval_estimates, int_sizes, error_rate_estimation_errors, majority_vote_estimations, weighted_vote_estimations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_workers: [ 3  5  7  9 11 15 21], evaluator: new\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:00<00:00, 47.91it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# num_workers/num_samples configurations to analyze\n",
    "num_workers = np.array([3, 5, 7, 9, 11, 15, 21])\n",
    "num_tasks_configs = [1000]\n",
    "iteration_count = 500\n",
    "evaluater_names = ['new', 'old greedy']\n",
    "confidence_level = 0.9\n",
    "\n",
    "# Stores whether the interval contained the true error rate for every dataset.\n",
    "correct_interval_estimates = dict()\n",
    "\n",
    "# Stores the average accuracy for every configuration.\n",
    "accuracies = dict()\n",
    "\n",
    "# Stores all measured interval sizes for every dataset\n",
    "interval_sizes = dict()\n",
    "\n",
    "# Stores the average interval sizes for every configuration\n",
    "average_interval_sizes = dict()\n",
    "\n",
    "# Stores all estimation error for every dataset\n",
    "estimation_errors = dict()\n",
    "\n",
    "# Stores the average estimation error for every configuration\n",
    "average_estimation_errors = dict()\n",
    "\n",
    "majority_vote_estimation_accuracies = dict()\n",
    "\n",
    "weighted_vote_estimation_accuracies = dict()\n",
    "\n",
    "for num_tasks in num_tasks_configs:\n",
    "    # n: num_samples\n",
    "    # m: num_workers\n",
    "    correct_interval_estimates[num_tasks] = dict()\n",
    "    accuracies[num_tasks] = dict()\n",
    "    interval_sizes[num_tasks] = dict()\n",
    "    average_interval_sizes[num_tasks] = dict()\n",
    "    estimation_errors[num_tasks] = dict()\n",
    "    average_estimation_errors[num_tasks] = dict()\n",
    "    majority_vote_estimation_accuracies[num_tasks] = dict()\n",
    "    weighted_vote_estimation_accuracies[num_tasks] = dict()\n",
    "\n",
    "    for t in evaluater_names:\n",
    "        print('num_workers: ' + str(num_workers) + ', evaluator: ' + t)\n",
    "        res = Parallel(n_jobs=7)(delayed(calc_fraction_of_wrong_interval_estimates)\n",
    "                (num_tasks, n, confidence_level, t, iteration_count, [0.1, 0.2, 0.3]) for n in tqdm(num_workers))\n",
    "\n",
    "        correct_interval_estimates_results = np.array([x[0] for x in res])\n",
    "        interval_sizes_results = np.array([x[1] for x in res])\n",
    "        est_errs_results = np.array([x[2] for x in res])\n",
    "        maj_vote_results = np.array([x[3] for x in res])\n",
    "        weighted_vote_results = np.array([x[4] for x in res])\n",
    "\n",
    "        accuracies[num_tasks][t] = np.array([np.sum(correct_interval_estimates_results[i]) for i in range(len(num_tasks))]) / (num_workers * iteration_count)\n",
    "        correct_interval_estimates[num_tasks][t] = correct_interval_estimates_results\n",
    "        interval_sizes[num_tasks][t] = interval_sizes_results\n",
    "        average_interval_sizes[num_tasks][t] = np.array([np.mean(interval_sizes_results[i]) for i in range(len(num_tasks))])\n",
    "        estimation_errors[num_tasks][t] = est_errs_results\n",
    "        average_estimation_errors[num_tasks][t] = np.array([np.mean(est_errs_results[i]) for i in range(len(num_tasks))])\n",
    "        majority_vote_estimation_accuracies[num_tasks][t] = np.array([np.mean(maj_vote_results[i]) for i in range(len(num_tasks))])\n",
    "        weighted_vote_estimation_accuracies[num_tasks][t] = np.array([np.mean(weighted_vote_results[i]) for i in range(len(num_tasks))])\n",
    "\n",
    "end = time.time()\n",
    "print('Time elapsed:', end-start)\n",
    "say(\"Accuracy and interval size vs num tasks completed\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mpl.rcParams['figure.dpi'] = 300\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "s = slice(0, len(num_workers))\n",
    "\n",
    "color = 'tab:blue'\n",
    "ax1.set_xlabel('number of workers')\n",
    "ax1.set_ylabel('accuracy')\n",
    "lns1 = ax1.plot(num_workers[s], accuracies[7]['new'][s], label='accuracy',\n",
    "         color=color, marker='s', linestyle='--', linewidth=1, markersize=2)\n",
    "\n",
    "ax2 = ax1.twinx()  # instantiate a second axes that shares the same x-axis\n",
    "\n",
    "color = 'tab:red'\n",
    "ax2.set_ylabel('average interval size')  # we already handled the x-label with ax1\n",
    "lns2 = ax2.plot(num_workers[s], average_interval_sizes[7]['new'][s], label='average interval size',\n",
    "         color=color, marker='s', linestyle='--', linewidth=1, markersize=2)\n",
    "\n",
    "lns = lns1 + lns2\n",
    "labs = [l.get_label() for l in lns]\n",
    "ax1.legend(lns, labs, loc=\"center right\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}