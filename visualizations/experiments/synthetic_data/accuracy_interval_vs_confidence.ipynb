{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from datasets import SyntheticDataset\n",
    "from crowd_evaluation import OldEvaluator, ConfidenceEvaluatorC, MajorityEvaluator\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "def say(msg = \"Finish\", voice = \"Victoria\"):\n",
    "    os.system(f'say -v {voice} {msg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from joblib import Memory, Parallel, delayed\n",
    "memory = Memory('accuracy_interval_vs_confidence', verbose=0)\n",
    "\n",
    "@memory.cache\n",
    "def calc_fraction_of_wrong_interval_estimates(\n",
    "        num_samples: int,\n",
    "        num_workers: int,\n",
    "        confidence: float,\n",
    "        evaluator_name: str,\n",
    "        iter_count: int):\n",
    "\n",
    "    correct_interval_estimates = np.zeros(iter_count * num_workers, dtype=np.float32)\n",
    "    error_rate_estimation_errors = np.zeros(iter_count * num_workers, dtype=np.float32)\n",
    "    int_sizes = np.zeros(iter_count * num_workers, dtype=np.float32)\n",
    "\n",
    "    for i in range(iter_count):\n",
    "        p_true = np.random.choice([0.1, 0.2, 0.3], size=num_workers)\n",
    "        dataset = SyntheticDataset(num_samples=num_samples, num_workers=num_workers, p_true=p_true)\n",
    "\n",
    "        if evaluator_name == 'old':\n",
    "            evaluator = OldEvaluator(dataset)\n",
    "            ps, confs = evaluator.evaluate_workers_with_confidence(\n",
    "                dataset.workers,\n",
    "                confidence=confidence,\n",
    "                method='exhaustive'\n",
    "            )\n",
    "        elif evaluator_name == 'old greedy':\n",
    "            evaluator = OldEvaluator(dataset)\n",
    "            ps, confs = evaluator.evaluate_workers_with_confidence(\n",
    "                dataset.workers,\n",
    "                confidence=confidence,\n",
    "                method='greedy'\n",
    "            )\n",
    "        elif evaluator_name == 'majority':\n",
    "            evaluator = MajorityEvaluator(dataset)\n",
    "            ps = evaluator.evaluate_workers(dataset.workers)\n",
    "            confs = np.zeros(num_workers)\n",
    "        else:\n",
    "            evaluator = ConfidenceEvaluatorC(dataset)\n",
    "            ps, confs = evaluator.evaluate_workers_with_confidence(\n",
    "                dataset.workers,\n",
    "                confidence=confidence,\n",
    "            )\n",
    "\n",
    "        min_limit, max_limit = ps-confs, ps+confs\n",
    "        correct_interval_estimates[i*num_workers:(i+1)*num_workers] = np.where((min_limit <= p_true) & (p_true <= max_limit), 1, 0)\n",
    "        int_sizes[i*num_workers:(i+1)*num_workers] = 2*confs\n",
    "        error_rate_estimation_errors[i*num_workers:(i+1)*num_workers] = np.abs(ps - p_true)\n",
    "\n",
    "    return correct_interval_estimates, int_sizes, error_rate_estimation_errors"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "confidence_levels = np.arange(0, 1, 0.05)\n",
    "\n",
    "# num_workers/num_samples configurations to analyze\n",
    "configurations = [(3, 300), (7, 300), (3, 100), (7, 100)]\n",
    "iteration_count = 500\n",
    "evaluater_names = ['new', 'old greedy']\n",
    "\n",
    "# Stores whether the interval contained the true error rate for every dataset.\n",
    "correct_interval_estimates = dict()\n",
    "\n",
    "# Stores the average accuracy for every configuration.\n",
    "accuracies = dict()\n",
    "\n",
    "# Stores all measured interval sizes for every dataset\n",
    "interval_sizes = dict()\n",
    "\n",
    "# Stores the average interval sizes for every configuration\n",
    "average_interval_sizes = dict()\n",
    "\n",
    "# Stores all estimation error for every dataset\n",
    "estimation_errors = dict()\n",
    "\n",
    "# Stores the average estimation error for every configuration\n",
    "average_estimation_errors = dict()\n",
    "\n",
    "for num_workers, num_samples in configurations:\n",
    "    # n: num_samples\n",
    "    # m: num_workers\n",
    "    correct_interval_estimates[(num_samples, num_workers)] = dict()\n",
    "    accuracies[(num_samples, num_workers)] = dict()\n",
    "    interval_sizes[(num_samples, num_workers)] = dict()\n",
    "    average_interval_sizes[(num_samples, num_workers)] = dict()\n",
    "    estimation_errors[(num_samples, num_workers)] = dict()\n",
    "    average_estimation_errors[(num_samples, num_workers)] = dict()\n",
    "\n",
    "    for t in evaluater_names:\n",
    "        print('num_workers: ' + str(num_workers) + ', num_tasks: ' + str(num_samples) + ', evaluator: ' + t)\n",
    "        res = Parallel(n_jobs=7)(delayed(calc_fraction_of_wrong_interval_estimates)\n",
    "                (num_samples, num_workers, c, t, iteration_count) for c in tqdm(confidence_levels))\n",
    "\n",
    "        correct_interval_estimates_results = np.array([x[0] for x in res])\n",
    "        interval_sizes_results = np.array([x[1] for x in res])\n",
    "        est_errs_results = np.array([x[2] for x in res])\n",
    "\n",
    "        accuracies[(num_samples, num_workers)][t] = np.array(\n",
    "            [np.sum(correct_interval_estimates_results[i]) for i in range(len(confidence_levels))]) / (num_workers * iteration_count)\n",
    "        correct_interval_estimates[(num_samples, num_workers)][t] = correct_interval_estimates_results\n",
    "        interval_sizes[(num_samples, num_workers)][t] = interval_sizes_results\n",
    "        average_interval_sizes[(num_samples, num_workers)][t] = np.array([np.mean(interval_sizes_results[i]) for i in range(len(confidence_levels))])\n",
    "        estimation_errors[(num_samples, num_workers)][t] = est_errs_results\n",
    "        average_estimation_errors[(num_samples, num_workers)][t] = np.array([np.mean(est_errs_results[i]) for i in range(len(confidence_levels))])\n",
    "\n",
    "say(\"Accuracy and interval size vs confidence calculations completed\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 600\n",
    "\n",
    "plt.plot(confidence_levels, np.linspace(0, 1, len(confidence_levels)), label='Ideal interval-accuracy', linewidth=1)\n",
    "for t in evaluater_names:\n",
    "    for num_workers, num_samples in configurations:\n",
    "        # n: num_samples\n",
    "        # m: num_workers\n",
    "        plt.plot(confidence_levels, accuracies[(num_samples, num_workers)][t], marker='s', linestyle='--',\n",
    "                 label =t +', ' + str(num_workers) + ' workers, ' + str(num_samples) + ' tasks', linewidth=1, markersize=2)\n",
    "\n",
    "plt.xlabel('Confidence level')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig('confidence_vs_accuracy.svg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 600\n",
    "\n",
    "plt.plot(confidence_levels, np.linspace(0, 1, len(confidence_levels)), label='Ideal interval-accuracy', linewidth=1)\n",
    "\n",
    "plt.plot(confidence_levels, accuracies[(300, 3)]['new'], marker='s', linestyle='--',\n",
    "             label = '3 workers, 300 tasks (new)', linewidth=1, markersize=2)\n",
    "# plt.plot(confidence_levels, accuracies[(300, 7)]['new'], marker='s', linestyle='--',\n",
    "#              label = '7 workers, 300 tasks', linewidth=1, markersize=2)\n",
    "plt.plot(confidence_levels, accuracies[(300, 3)]['old greedy'], marker='s', linestyle='--',\n",
    "             label = '3 workers, 300 tasks (old)', linewidth=1, markersize=2)\n",
    "\n",
    "plt.plot(confidence_levels, accuracies[(100, 3)]['new'], marker='s', linestyle='--',\n",
    "             label = '3 workers, 100 tasks (new)', linewidth=1, markersize=2)\n",
    "plt.plot(confidence_levels, accuracies[(100, 3)]['old greedy'], marker='s', linestyle='--',\n",
    "             label = '3 workers, 100 tasks (old)', linewidth=1, markersize=2)\n",
    "\n",
    "# plt.plot(confidence_levels, confidence_levels - accuracies[(300, 7)]['new'], marker='s', linestyle='--',\n",
    "#              label = '7 workers, 300 tasks', linewidth=1, markersize=2)\n",
    "\n",
    "plt.xlabel('Confidence level')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(confidence_levels, np.linspace(0, 1, len(confidence_levels)), label='Ideal interval-accuracy', linewidth=1)\n",
    "\n",
    "plt.plot(confidence_levels, accuracies[(300, 7)]['new'], marker='s', linestyle='--',\n",
    "             label = '7 workers, 300 tasks (new)', linewidth=1, markersize=2)\n",
    "plt.plot(confidence_levels, accuracies[(300, 7)]['old greedy'], marker='s', linestyle='--',\n",
    "             label = '7 workers, 300 tasks (old)', linewidth=1, markersize=2)\n",
    "\n",
    "plt.plot(confidence_levels, accuracies[(100, 7)]['new'], marker='s', linestyle='--',\n",
    "             label = '7 workers, 100 tasks (new)', linewidth=1, markersize=2)\n",
    "plt.plot(confidence_levels, accuracies[(100, 7)]['old greedy'], marker='s', linestyle='--',\n",
    "             label = '7 workers, 100 tasks (old)', linewidth=1, markersize=2)\n",
    "\n",
    "# plt.plot(confidence_levels, confidence_levels - accuracies[(300, 7)]['new'], marker='s', linestyle='--',\n",
    "#              label = '7 workers, 300 tasks', linewidth=1, markersize=2)\n",
    "\n",
    "plt.xlabel('Confidence level')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for t in evaluater_names:\n",
    "    for num_workers, num_samples in [(3, 100), (7,100)]:\n",
    "        # n: num_samples\n",
    "        # m: num_workers\n",
    "        plt.plot(confidence_levels, average_interval_sizes[(num_samples, num_workers)][t], marker='s', linestyle='--',\n",
    "                 label =t +',' + str(num_workers) + ' workers, ' + str(num_samples) + ' tasks', linewidth=1, markersize=2)\n",
    "\n",
    "plt.xlabel('Confidence level')\n",
    "plt.ylabel('Interval size')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "# plt.savefig('confidence_vs_interval_size.svg')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "result = np.array(interval_sizes[(300, 3)]['new'][1:]) / np.array(interval_sizes[(300, 3)]['old greedy'][1:])\n",
    "np.mean(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(confidence_levels, average_interval_sizes[(100, 7)]['new'], marker='s', linestyle='--',\n",
    "                 label ='7 workers, 100 tasks (new)', linewidth=1, markersize=2)\n",
    "plt.plot(confidence_levels, average_interval_sizes[(300, 7)]['new'], marker='s', linestyle='--',\n",
    "                 label ='7 workers, 300 tasks (new)', linewidth=1, markersize=2)\n",
    "\n",
    "plt.plot(confidence_levels, average_interval_sizes[(300, 7)]['old greedy'], marker='s', linestyle='--',\n",
    "                 label ='7 workers, 300 tasks (old)', linewidth=1, markersize=2)\n",
    "plt.plot(confidence_levels, average_interval_sizes[(100, 7)]['old greedy'], marker='s', linestyle='--',\n",
    "                 label ='7 workers, 100 tasks (old)', linewidth=1, markersize=2)\n",
    "\n",
    "plt.xlabel('Confidence Level')\n",
    "plt.ylabel('Average Interval Size')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('confidence_vs_interval_size_7_workers.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plt.plot(confidence_levels, average_interval_sizes[(100, 3)]['new'], marker='s', linestyle='--',\n",
    "                 label ='3 workers, 100 tasks (new)', linewidth=1, markersize=2)\n",
    "plt.plot(confidence_levels, average_interval_sizes[(300, 3)]['new'], marker='s', linestyle='--',\n",
    "                 label ='3 workers, 300 tasks (new)', linewidth=1, markersize=2)\n",
    "\n",
    "plt.plot(confidence_levels, average_interval_sizes[(300, 3)]['old greedy'], marker='s', linestyle='--',\n",
    "                 label ='3 workers, 300 tasks (old)', linewidth=1, markersize=2)\n",
    "plt.plot(confidence_levels, average_interval_sizes[(100, 3)]['old greedy'], marker='s', linestyle='--',\n",
    "                 label ='3 workers, 100 tasks (old)', linewidth=1, markersize=2)\n",
    "\n",
    "plt.xlabel('Confidence Level')\n",
    "plt.ylabel('Average Interval Size')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.savefig('confidence_vs_interval_size_3_workers.png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "np.mean(estimation_errors[(100, 3)]['old greedy'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 600\n",
    "\n",
    "for t in evaluater_names:\n",
    "    for num_workers, num_samples in configurations:\n",
    "        # n: num_samples\n",
    "        # m: num_workers\n",
    "        plt.plot(confidence_levels, estimation_errors[(num_samples, num_workers)][t], marker='s', linestyle='--',\n",
    "                 label =t +', ' + str(num_workers) + ' workers, ' + str(num_samples) + ' tasks', linewidth=1, markersize=2)\n",
    "\n",
    "plt.xlabel('Confidence level')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# %%timeit -r 1 -n 1\n",
    "#\n",
    "# num_workers = 3\n",
    "# p_true = np.random.choice([0.1, 0.08, 0.06], p=[0.3, 0.4, 0.3], size=num_workers)\n",
    "# dataset = SyntheticDataset(num_samples=50, num_workers=num_workers, p_true=p_true)\n",
    "# evaluator = OldEvaluator(dataset, debug=False)\n",
    "#\n",
    "# for i in range(0, 1000):\n",
    "#     ps_exhaustive, confs_exhaustive = evaluator.evaluate_workers_with_confidence(\n",
    "#         dataset.workers,\n",
    "#         confidence=0.9,\n",
    "#         method='exhaustive',\n",
    "#     )\n",
    "#\n",
    "# ps_greedy, confs_greedy = evaluator.evaluate_workers_with_confidence(\n",
    "#     dataset.workers,\n",
    "#     confidence=0.9,\n",
    "#     method='greedy',\n",
    "# )\n",
    "#\n",
    "# from visualizations.utils import visualize_error_rates\n",
    "#\n",
    "# visualize_error_rates(\n",
    "#     dataset=dataset,\n",
    "#     workers=dataset.workers,\n",
    "#     p_ests=[ps_exhaustive, ps_greedy],\n",
    "#     confs=[confs_exhaustive, confs_greedy],\n",
    "#     labels=[r'$p_{est, exhaustive}$', r'$p_{est, greedy}$'],\n",
    "# )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}