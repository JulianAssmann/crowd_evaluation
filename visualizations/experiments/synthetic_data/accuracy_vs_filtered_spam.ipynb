{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from joblib import Memory, Parallel, delayed\n",
    "memory = Memory('accuracy_vs_spam', verbose=0)\n",
    "\n",
    "@memory.cache\n",
    "def calc_fraction_of_wrong_interval_estimates(\n",
    "        num_samples: int,\n",
    "        num_workers: int,\n",
    "        confidence: float,\n",
    "        evaluator_name: str,\n",
    "        iter_count: int,\n",
    "        error_rates: list[float],\n",
    "        spammer_error_rates: list[float],\n",
    "        spam_level: float):\n",
    "\n",
    "    correct_interval_estimates = np.zeros(iter_count * num_workers, dtype=np.float32)\n",
    "    error_rate_estimation_errors = np.zeros(iter_count * num_workers, dtype=np.float32)\n",
    "    int_sizes = np.zeros(iter_count * num_workers, dtype=np.float32)\n",
    "\n",
    "    # Saves for every sample generated whether the\n",
    "    # majority vote estimates the label correctly (=1) or not (=0)\n",
    "    majority_vote_estimations = np.zeros(iter_count * num_samples)\n",
    "\n",
    "    # Saves for every sample generated whether the\n",
    "    # weighted vote estimates the label correctly (=1) or not (=0)\n",
    "    weighted_vote_estimations = np.zeros(iter_count * num_samples)\n",
    "\n",
    "    for i in range(iter_count):\n",
    "        spammer_p_true = np.random.choice(spammer_error_rates, size=num_workers)\n",
    "        normal_p_true = np.random.choice(error_rates, size=num_workers)\n",
    "        p_true = np.where(np.random.choice([1, 0], num_workers, p=[spam_level, 1-spam_level]) == 1, spammer_p_true, normal_p_true)\n",
    "        dataset = SyntheticDataset(num_samples=num_samples, num_workers=num_workers, p_true=p_true)\n",
    "\n",
    "        if evaluator_name == 'old':\n",
    "            evaluator = ConfidenceEvaluatorOld(dataset)\n",
    "            ps, confs = evaluator.evaluate_workers_with_confidence(\n",
    "                dataset.workers,\n",
    "                confidence=confidence,\n",
    "                method='exhaustive'\n",
    "            )\n",
    "        elif evaluator_name == 'old greedy':\n",
    "            evaluator = ConfidenceEvaluatorOld(dataset)\n",
    "            ps, confs = evaluator.evaluate_workers_with_confidence(\n",
    "                dataset.workers,\n",
    "                confidence=confidence,\n",
    "                method='greedy'\n",
    "            )\n",
    "        elif evaluator_name == 'majority':\n",
    "            evaluator = MajorityEvaluator(dataset)\n",
    "            ps = evaluator.evaluate_workers(dataset.workers)\n",
    "            confs = np.zeros(num_workers)\n",
    "        else:\n",
    "            evaluator = ConfidenceEvaluatorNew(dataset)\n",
    "            ps, confs = evaluator.evaluate_workers_with_confidence(\n",
    "                dataset.workers,\n",
    "                confidence=confidence,\n",
    "            )\n",
    "\n",
    "        ground_truth = dataset.get_ground_truth_for_samples(dataset.samples)\n",
    "        maj_vote = VoteAggregator.majority_vote(dataset, dataset.samples)\n",
    "        weighted_vote = VoteAggregator.weighted_vote(dataset, dataset.samples, ps, 0.5)\n",
    "\n",
    "        min_limit, max_limit = ps-confs, ps+confs\n",
    "        correct_interval_estimates[i*num_workers:(i+1)*num_workers] = np.where((min_limit <= p_true) & (p_true <= max_limit), 1, 0)\n",
    "        int_sizes[i*num_workers:(i+1)*num_workers] = 2*confs\n",
    "        error_rate_estimation_errors[i*num_workers:(i+1)*num_workers] = np.abs(ps - p_true)\n",
    "\n",
    "        majority_vote_estimations[i*num_samples:(i+1)*num_samples] = (ground_truth == maj_vote)\n",
    "        weighted_vote_estimations[i*num_samples:(i+1)*num_samples] = (ground_truth == weighted_vote)\n",
    "\n",
    "    return correct_interval_estimates, int_sizes, error_rate_estimation_errors, \\\n",
    "           majority_vote_estimations, weighted_vote_estimations"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from datasets import SyntheticDataset\n",
    "from crowd_evaluation import ConfidenceEvaluatorNew, ConfidenceEvaluatorOld, MajorityEvaluator, VoteAggregator\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi'] = 300"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}